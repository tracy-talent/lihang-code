{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章 隐马尔可夫模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的序列的过程。\n",
    "\n",
    "隐马尔可夫模型由初始状态概率向$\\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。因此，隐马尔可夫模型可以写成$\\lambda=(A, B, \\pi)$。\n",
    "\n",
    "隐马尔可夫模型是一个生成模型，表示状态序列和观测序列的联合分布，但是状态序列是隐藏的，不可观测的。\n",
    "\n",
    "隐马尔可夫模型可以用于标注，这时状态对应着标记。标注问题是给定观测序列预测其对应的标记序列。\n",
    "\n",
    "2．概率计算问题。给定模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，计算在模型$\\lambda$下观测序列$O$出现的概率$P(O|\\lambda)$。前向-后向算法是通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。\n",
    " \n",
    "3．学习问题。已知观测序列$O＝(o_1，o_2,…,o_T)$，估计模型$\\lambda=(A, B, \\pi)$参数，使得在该模型下观测序列概率$P(O|\\lambda)$最大。即用极大似然估计的方法估计参数。Baum-Welch算法，也就是EM算法可以高效地对隐马尔可夫模型进行训练。它是一种非监督学习算法。\n",
    "\n",
    "4．预测问题。已知模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I＝(i_1，i_2,…,i_T)$。维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 公式：<br>\n",
    "前向概率：\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\alpha_{1}(i)=\\pi_{i}b_{i}(o_{1})\\tag{1.1} \\\\ \n",
    "\\alpha_{t+1}(i)=[\\sum_{j=1}^{N}\\alpha_{t}(j)a_{ji}]b_{i}(o_{t+1})\\tag{1.2} \\\\\n",
    "P(O|\\lambda) = \\sum_{i=1}^{N}\\alpha_{T}(i)\\tag{1.3}\n",
    "\\end{gather*}\n",
    "$$\n",
    "后向概率：\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\beta_{T}(i)=1\\tag{2.1} \\\\\n",
    "\\beta_t(i)=\\sum_{j=1}^{N}[a_{ij}b_{j}(o_{t+1})\\beta_{t+1}(j)]\\tag{2.2} \\\\\n",
    "P(O|\\lambda) = \\sum_{i=1}^{N}[\\pi_{i}b_{i}(o_{1})\\beta_{1}(i)]\\tag{2.3}\n",
    "\\end{gather*}\n",
    "$$\n",
    "两个状态的概率计算公式：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\xi_{t}(i,j)&=P(i_{t}=q_{i},i_{t+1}=q_{j}|O,\\lambda)\\\\\n",
    "&=\\frac{P(i_{t}=q_{i},i_{t+1}=q_{j},O|\\lambda)}{P(O|\\lambda)}\\\\\n",
    "&=\\frac{P(i_{t}=q_{i},i_{t+1}=q_{j},O|\\lambda)}{\\sum_{i=1}^{N}\\sum_{j=1}^{N}P(i_{t}=q_{i},i_{t+1}=q_{j},O|\\lambda)}\\\\\n",
    "&=\\frac{\\alpha_{t}(i)a_{ij}b_{j}(o_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_{t}(i)a_{ij}b_{j}(o_{t+1})\\beta_{t+1}(j)}\n",
    "\\end{split} \\tag{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "单个状态的概率计算公式：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\gamma_{t}(i)&=P(i_{t}=q_{i}|O,\\lambda)\\\\\n",
    "&=\\frac{P(i_{t}=q_{i},O|\\lambda)}{P(O|\\lambda)}\\\\\n",
    "&=\\frac{P(i_{t}=q_{i},O|\\lambda)}{\\sum_{j=1}^{T}P(i_{t}=q_{j},O|\\lambda)}\\\\\n",
    "&=\\frac{\\alpha_{t}(i)\\beta_{t}(i)}{\\sum_{j=1}^{N}\\alpha_{t}(j)\\beta_{t}(j)}\n",
    "\\end{split}\\tag{4}\n",
    "\\end{equation}\n",
    "$$\n",
    "EM算法对Baum-Welch模型参数估计公式：\n",
    "$$\n",
    "\\begin{gather*}\n",
    "a_{ij}=\\frac{\\sum_{t=1}^{T-1}\\xi_{t}(i,j)}{\\sum_{t=1}^{T-1}\\gamma_{t}(i)}\\tag{5.1} \\\\\n",
    "b_{j}(k)=\\frac{\\sum_{t=1,o_{t}=v_{k}}^{T}\\gamma_{t}(j)}{\\sum_{t=1}^{T}\\gamma_{t}(j)}\\tag{5.2}\\\\\n",
    "\\pi_{i}=\\gamma_{1}(i)\\tag{5.3}\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=5>Cautions:</font><br>\n",
    "即使使用Baum-Welch算法也无法做到完全无监督的训练。比如要预测单词词性并且没有标注语料集，那么必须具备<br>\n",
    "* 词性集合\n",
    "* 词典\n",
    "\n",
    "根据词性集合和词典构建初始的观测概率分布矩阵，构建状态到词性的映射，状态转移矩阵可使用高斯分布随机初始化。<br>\n",
    "概率直接相乘可能会浮点下溢，可以取对数计算，但是对数不能出现0概率，所以需要对概率进行平滑处理。<br>\n",
    "对于每个输入的观测序列都需要使用EM算法对参数重新进行训练，然后在使用viterbi算法输出预测的状态序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkov:\n",
    "    def __init__(self, Q, V, A, B, PI):\n",
    "        self.Q = Q  # 状态数组或者状态映射也可以，N\n",
    "        self.V = np.array(V) # 观测集数组，包含数据集中所有可能的观测项，T\n",
    "        self.A = np.array(A) # 状态转移概率分布矩阵，N*N\n",
    "        self.B = np.array(B) # 观测概率分布矩阵，N*T\n",
    "        self.PI = np.array(PI) # 初始状态概率分布数组，N\n",
    "        \n",
    "    def forward(self, O, logs=False):  # 使用前向算法，O为观察序列，logs用于控制是否输出计算过程\n",
    "        N = len(self.Q)  #可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        self.alphas = np.zeros((N, M))  # 前向概率：alphas[i][j]表示t时刻部分观测序列为o1,o2,o3...,ot且状态为qi的概率\n",
    "        T = M  # 有几个时刻，有几个观测序列，就有几个时刻\n",
    "        for t in range(T):  # 遍历每一时刻，算出alpha值\n",
    "            indexOfO = np.where(self.V == O[t])[0][0]  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                if t == 0:  # 计算初值\n",
    "                    self.alphas[i][t] = self.PI[i] * self.B[i][indexOfO]  # P176（10.15）\n",
    "                    if logs:\n",
    "                        print('alpha1(%d)=p%db%db(o1)=%f' % (i, i, i, self.alphas[i][t]))\n",
    "                else:\n",
    "                    self.alphas[i][t] = np.dot(\n",
    "                        [alpha[t - 1] for alpha in self.alphas],\n",
    "                        [a[i] for a in self.A]) * self.B[i][indexOfO]  # 对应P176（10.16）\n",
    "                    if logs:\n",
    "                        print('alpha%d(%d)=sigma [alpha%d(i)ai%d]b%d(o%d)=%f' %\n",
    "                              (t, i, t - 1, i, i, t, self.alphas[i][t]))\n",
    "                        # print(alphas)\n",
    "        P = np.sum([alpha[M - 1] for alpha in self.alphas])  # P176(10.17)\n",
    "        if logs:\n",
    "            print(\"P(O|lambda)=\", end=\"\")\n",
    "            for i in range(N):\n",
    "                print(\"%.3f+\" % self.alphas[i][M - 1], end=\"\")\n",
    "            print(\"0=%.6f\" % P)\n",
    "        # alpha11 = pi[0][0] * B[0][0]    #代表a1(1)\n",
    "        # alpha12 = pi[0][1] * B[1][0]    #代表a1(2)\n",
    "        # alpha13 = pi[0][2] * B[2][0]    #代表a1(3)\n",
    "\n",
    "    def backward(self, O, logs=False):  # 后向算法，O为观察序列,logs用于控制是否输出计算过程\n",
    "        N = len(self.Q)  # 可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        self.betas = np.ones((N, M))  # 后向概率：时刻t状态为qi的条件下，从t+1到T的部分观测序列为ot+1,ot+2,...,oT的概率\n",
    "        if logs:\n",
    "            for i in range(N):\n",
    "                print('beta%d(%d)=1' % (M, i))\n",
    "        for t in range(M - 2, -1, -1):\n",
    "            indexOfO = np.where(self.V == O[t + 1])[0][0]  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                self.betas[i][t] = np.dot(\n",
    "                    np.multiply(self.A[i], [b[indexOfO] for b in self.B]),\n",
    "                    [beta[t + 1] for beta in self.betas])\n",
    "                realT = t + 1\n",
    "                realI = i + 1\n",
    "                if logs:\n",
    "                    print(\n",
    "                        'beta%d(%d)=sigma [a%djbj(o%d)beta%d(j)]=(' %\n",
    "                        (realT, realI, realI, realT + 1, realT + 1),\n",
    "                        end='')\n",
    "                    for j in range(N):\n",
    "                        print(\n",
    "                            \"%.3f*%.3f*%.3f+\" % (self.A[i][j], self.B[j][indexOfO],\n",
    "                                                 self.betas[j][t + 1]),\n",
    "                            end='')\n",
    "                    print(\"0)=%.6f\" % self.betas[i][t])\n",
    "        # print(betas)\n",
    "        if logs:\n",
    "            indexOfO = np.where(self.V == O[0])[0][0]\n",
    "            P = np.dot(\n",
    "                np.multiply(self.PI, [b[indexOfO] for b in self.B]),\n",
    "                [beta[0] for beta in self.betas])\n",
    "            print(\"P(O|lambda)=\", end=\"\")\n",
    "            for i in range(N):\n",
    "                print(\n",
    "                    \"%.3f*%.3f*%.3f+\" % (self.PI[i], self.B[i][indexOfO], self.betas[i][0]),\n",
    "                    end=\"\")\n",
    "            print(\"0=%.6f\" % P)\n",
    "\n",
    "    def viterbi(self, O, logs=False): # viterbi算法进行状态decode，O为观测序列，logs用于控制是否输出计算过程\n",
    "        N = len(self.Q)  #可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        self.deltas = np.zeros((N, M)) # deltas[i][t]表示t时刻状态为qi的所有状态序列中的最大概率\n",
    "        self.psis = np.zeros((N, M)) # psis[i][t]使t时刻状态为qi最大化的t-1时刻的状态\n",
    "        I = np.zeros(M, dtype=np.int32)\n",
    "        for t in range(M):\n",
    "            realT = t + 1\n",
    "            indexOfO = np.where(self.V == O[t])[0][0]  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                realI = i + 1\n",
    "                if t == 0:\n",
    "                    self.deltas[i][t] = self.PI[i] * self.B[i][indexOfO]\n",
    "                    self.psis[i][t] = 0\n",
    "                    if logs:\n",
    "                        print('delta1(%d)=pi%d * b%d(o1)=%.3f * %.3f=%.6f' %\n",
    "                              (realI, realI, realI, self.PI[i], self.B[i][indexOfO],\n",
    "                               self.deltas[i][t]))\n",
    "                        print('psis1(%d)=0' % (realI))\n",
    "                else:\n",
    "                    self.deltas[i][t] = np.max(\n",
    "                        np.multiply([delta[t - 1] for delta in self.deltas],\n",
    "                                    [a[i] for a in self.A])) * self.B[i][indexOfO]\n",
    "                    if logs:\n",
    "                        print(\n",
    "                            'delta%d(%d)=max[delta%d(j)aj%d]b%d(o%d)=%.3f*%.3f=%.6f'\n",
    "                            % (realT, realI, realT - 1, realI, realI, realT,\n",
    "                               np.max(\n",
    "                                   np.multiply([delta[t - 1] for delta in self.deltas],\n",
    "                                               [a[i] for a in self.A])), self.B[i][indexOfO],\n",
    "                               self.deltas[i][t]))\n",
    "                    self.psis[i][t] = np.argmax(\n",
    "                        np.multiply(\n",
    "                            [delta[t - 1] for delta in self.deltas],\n",
    "                            [a[i]\n",
    "                             for a in self.A])) + 1  #由于其返回的是索引，因此应+1才能和正常的下标值相符合。\n",
    "                    if logs:\n",
    "                        print('psis%d(%d)=argmax[delta%d(j)aj%d]=%d' %\n",
    "                              (realT, realI, realT - 1, realI, self.psis[i][t]))\n",
    "        if logs:\n",
    "            print(self.deltas)\n",
    "            print(self.psis)\n",
    "        I[M - 1] = np.argmax([delta[M - 1] for delta in self.deltas\n",
    "                                 ]) + 1  #由于其返回的是索引，因此应+1才能和正常的下标值相符合。\n",
    "        print('i%d=argmax[deltaT(i)]=%d' % (M, I[M - 1]))\n",
    "        for t in range(M - 2, -1, -1):\n",
    "            I[t] = self.psis[int(I[t + 1]) - 1][t + 1]\n",
    "            print('i%d=psis%d(i%d)=%d' % (t + 1, t + 2, t + 2, I[t]))\n",
    "        print(\"状态序列I：\", I)\n",
    "        return I\n",
    "    \n",
    "    def train(self, O, criterion=0.05, logs=False): \n",
    "        '''\n",
    "        Baum-Welch无监督参数学习，EM算法进行训练，训练之前必须已计算前向forward和后向backward概率\n",
    "        O为观察序列\n",
    "        criterion为前后两次训练参数相差允许的最小值，用于控制迭代次数\n",
    "        logs为真则会打印forward和backward详细的计算过程\n",
    "        '''\n",
    "        N = len(self.Q)\n",
    "        M = len(O)\n",
    "        xi = np.zeros((M - 1, N, N))\n",
    "        gamma = np.zeros((M, N))\n",
    "        done = False\n",
    "        O_index = np.zeros(M, dtype=np.int32)\n",
    "        for t in range(M):\n",
    "            O_index[t] = np.where(self.V == O[t])[0][0]\n",
    "        while not done:\n",
    "            # 计算更新参数后的前向概率alphas和后向概率betas\n",
    "            self.forward(O,logs=logs)\n",
    "            self.backward(O,logs=logs)\n",
    "            # EM算法的E step\n",
    "            # 计算xi\n",
    "            for t in range(M - 1):\n",
    "                indexofO = O_index[t + 1]\n",
    "                xi_divisor = np.dot([alpha[t] for alpha in self.alphas], \n",
    "                                    [np.dot(np.multiply(self.A[i], [b[indexofO] for b in self.B]), \n",
    "                                            [beta[t + 1] for beta in self.betas]) for i in range(N)])\n",
    "                xi_dividend = np.array([self.alphas[i][t] * np.multiply(np.multiply(self.A[i], [b[indexofO] for b in self.B]), \n",
    "                                                  [beta[t + 1] for beta in self.betas]) for i in range(N)])\n",
    "                xi[t] = xi_dividend / xi_divisor\n",
    "            # 计算gamma\n",
    "            for t in range(M):\n",
    "                gamma[t] = np.multiply([alpha[t] for alpha in self.alphas], [beta[t] for beta in self.betas]) / np.dot(\n",
    "                    [alpha[t] for alpha in self.alphas], [beta[t] for beta in self.betas])\n",
    "            # EM算法的M step\n",
    "            # 更新状态转移概率分布矩阵A\n",
    "            new_A = np.zeros(self.A.shape)\n",
    "            for i in range(N):\n",
    "                new_A_divisor = np.sum([g[i] for g in gamma]) - gamma[M - 1][i]\n",
    "                for j in range(N):\n",
    "                    new_A[i][j] = np.sum([xit[i, j] for xit in xi]) / new_A_divisor\n",
    "            # 更新观测概率分布矩阵B\n",
    "            new_B = np.zeros(self.B.shape)\n",
    "            for j in range(new_B.shape[0]):\n",
    "                new_B_divisor = np.sum([g[j] for g in gamma])\n",
    "                for k in range(new_B.shape[1]):\n",
    "                    new_B[j][k] = np.sum([gamma[t][j] for t in range(M) if O_index[t] == k]) / new_B_divisor\n",
    "            # 更新初始状态概率分布数组PI\n",
    "            new_PI = np.zeros(self.PI.shape)\n",
    "            new_PI = gamma[0]\n",
    "            # 对比前后两次更新幅度\n",
    "            if (np.max(np.abs(new_A - self.A)) < criterion \n",
    "                and np.max(np.abs(new_B - self.B)) < criterion \n",
    "                and np.max(np.abs(new_PI - self.PI)) < criterion):\n",
    "                done = True\n",
    "            self.A[:,:], self.B[:,:], self.PI[:] = new_A, new_B, new_PI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#习题10.1\n",
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = [[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]\n",
    "B = [[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]]\n",
    "# O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "O = ['红', '白', '红', '白']    #习题10.1的例子\n",
    "PI = [0.2, 0.4, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i4=argmax[deltaT(i)]=2\n",
      "i3=psis4(i4)=2\n",
      "i2=psis3(i3)=2\n",
      "i1=psis2(i2)=3\n",
      "状态序列I： [3 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMM = HiddenMarkov(Q, V, A, B, PI)\n",
    "# HMM.forward(O)\n",
    "# HMM.backward(Q)\n",
    "HMM.viterbi(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = [[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]]\n",
    "B = [[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]]\n",
    "O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "PI = [0.2, 0.3, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward:\n",
      "alpha1(0)=p0b0b(o1)=0.100000\n",
      "alpha1(1)=p1b1b(o1)=0.120000\n",
      "alpha1(2)=p2b2b(o1)=0.350000\n",
      "alpha1(0)=sigma [alpha0(i)ai0]b0(o1)=0.078000\n",
      "alpha1(1)=sigma [alpha0(i)ai1]b1(o1)=0.111000\n",
      "alpha1(2)=sigma [alpha0(i)ai2]b2(o1)=0.068700\n",
      "alpha2(0)=sigma [alpha1(i)ai0]b0(o2)=0.043020\n",
      "alpha2(1)=sigma [alpha1(i)ai1]b1(o2)=0.036684\n",
      "alpha2(2)=sigma [alpha1(i)ai2]b2(o2)=0.055965\n",
      "alpha3(0)=sigma [alpha2(i)ai0]b0(o3)=0.021854\n",
      "alpha3(1)=sigma [alpha2(i)ai1]b1(o3)=0.017494\n",
      "alpha3(2)=sigma [alpha2(i)ai2]b2(o3)=0.033758\n",
      "alpha4(0)=sigma [alpha3(i)ai0]b0(o4)=0.011463\n",
      "alpha4(1)=sigma [alpha3(i)ai1]b1(o4)=0.013947\n",
      "alpha4(2)=sigma [alpha3(i)ai2]b2(o4)=0.008080\n",
      "alpha5(0)=sigma [alpha4(i)ai0]b0(o5)=0.005766\n",
      "alpha5(1)=sigma [alpha4(i)ai1]b1(o5)=0.004676\n",
      "alpha5(2)=sigma [alpha4(i)ai2]b2(o5)=0.007188\n",
      "alpha6(0)=sigma [alpha5(i)ai0]b0(o6)=0.002862\n",
      "alpha6(1)=sigma [alpha5(i)ai1]b1(o6)=0.003389\n",
      "alpha6(2)=sigma [alpha5(i)ai2]b2(o6)=0.001878\n",
      "alpha7(0)=sigma [alpha6(i)ai0]b0(o7)=0.001411\n",
      "alpha7(1)=sigma [alpha6(i)ai1]b1(o7)=0.001698\n",
      "alpha7(2)=sigma [alpha6(i)ai2]b2(o7)=0.000743\n",
      "P(O|lambda)=0.001+0.002+0.001+0=0.003852\n",
      "\n",
      "backward:\n",
      "beta8(0)=1\n",
      "beta8(1)=1\n",
      "beta8(2)=1\n",
      "beta7(1)=sigma [a1jbj(o8)beta8(j)]=(0.500*0.500*1.000+0.200*0.600*1.000+0.300*0.300*1.000+0)=0.460000\n",
      "beta7(2)=sigma [a2jbj(o8)beta8(j)]=(0.300*0.500*1.000+0.500*0.600*1.000+0.200*0.300*1.000+0)=0.510000\n",
      "beta7(3)=sigma [a3jbj(o8)beta8(j)]=(0.200*0.500*1.000+0.300*0.600*1.000+0.500*0.300*1.000+0)=0.430000\n",
      "beta6(1)=sigma [a1jbj(o7)beta7(j)]=(0.500*0.500*0.460+0.200*0.600*0.510+0.300*0.300*0.430+0)=0.214900\n",
      "beta6(2)=sigma [a2jbj(o7)beta7(j)]=(0.300*0.500*0.460+0.500*0.600*0.510+0.200*0.300*0.430+0)=0.247800\n",
      "beta6(3)=sigma [a3jbj(o7)beta7(j)]=(0.200*0.500*0.460+0.300*0.600*0.510+0.500*0.300*0.430+0)=0.202300\n",
      "beta5(1)=sigma [a1jbj(o6)beta6(j)]=(0.500*0.500*0.215+0.200*0.400*0.248+0.300*0.700*0.202+0)=0.116032\n",
      "beta5(2)=sigma [a2jbj(o6)beta6(j)]=(0.300*0.500*0.215+0.500*0.400*0.248+0.200*0.700*0.202+0)=0.110117\n",
      "beta5(3)=sigma [a3jbj(o6)beta6(j)]=(0.200*0.500*0.215+0.300*0.400*0.248+0.500*0.700*0.202+0)=0.122031\n",
      "beta4(1)=sigma [a1jbj(o5)beta5(j)]=(0.500*0.500*0.116+0.200*0.600*0.110+0.300*0.300*0.122+0)=0.053205\n",
      "beta4(2)=sigma [a2jbj(o5)beta5(j)]=(0.300*0.500*0.116+0.500*0.600*0.110+0.200*0.300*0.122+0)=0.057762\n",
      "beta4(3)=sigma [a3jbj(o5)beta5(j)]=(0.200*0.500*0.116+0.300*0.600*0.110+0.500*0.300*0.122+0)=0.049729\n",
      "beta3(1)=sigma [a1jbj(o4)beta4(j)]=(0.500*0.500*0.053+0.200*0.400*0.058+0.300*0.700*0.050+0)=0.028365\n",
      "beta3(2)=sigma [a2jbj(o4)beta4(j)]=(0.300*0.500*0.053+0.500*0.400*0.058+0.200*0.700*0.050+0)=0.026495\n",
      "beta3(3)=sigma [a3jbj(o4)beta4(j)]=(0.200*0.500*0.053+0.300*0.400*0.058+0.500*0.700*0.050+0)=0.029657\n",
      "beta2(1)=sigma [a1jbj(o3)beta3(j)]=(0.500*0.500*0.028+0.200*0.400*0.026+0.300*0.700*0.030+0)=0.015439\n",
      "beta2(2)=sigma [a2jbj(o3)beta3(j)]=(0.300*0.500*0.028+0.500*0.400*0.026+0.200*0.700*0.030+0)=0.013706\n",
      "beta2(3)=sigma [a3jbj(o3)beta3(j)]=(0.200*0.500*0.028+0.300*0.400*0.026+0.500*0.700*0.030+0)=0.016396\n",
      "beta1(1)=sigma [a1jbj(o2)beta2(j)]=(0.500*0.500*0.015+0.200*0.600*0.014+0.300*0.300*0.016+0)=0.006980\n",
      "beta1(2)=sigma [a2jbj(o2)beta2(j)]=(0.300*0.500*0.015+0.500*0.600*0.014+0.200*0.300*0.016+0)=0.007411\n",
      "beta1(3)=sigma [a3jbj(o2)beta2(j)]=(0.200*0.500*0.015+0.300*0.600*0.014+0.500*0.300*0.016+0)=0.006470\n",
      "P(O|lambda)=0.200*0.500*0.007+0.300*0.400*0.007+0.500*0.700*0.006+0=0.003852\n",
      "\n",
      "viterbi:\n",
      "i8=argmax[deltaT(i)]=2\n",
      "i7=psis8(i8)=2\n",
      "i6=psis7(i7)=2\n",
      "i5=psis6(i6)=2\n",
      "i4=psis5(i5)=3\n",
      "i3=psis4(i4)=3\n",
      "i2=psis3(i3)=3\n",
      "i1=psis2(i2)=3\n",
      "状态序列I： [3 3 3 3 2 2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 2, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMM = HiddenMarkov(Q, V, A, B, PI)\n",
    "print('forward:')\n",
    "HMM.forward(O, logs=True)\n",
    "print('\\nbackward:')\n",
    "HMM.backward(O, logs=True)\n",
    "print('\\nviterbi:')\n",
    "HMM.viterbi(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "viterbi:\n",
      "i8=argmax[deltaT(i)]=2\n",
      "i7=psis8(i8)=2\n",
      "i6=psis7(i7)=3\n",
      "i5=psis6(i6)=2\n",
      "i4=psis5(i5)=2\n",
      "i3=psis4(i4)=3\n",
      "i2=psis3(i3)=2\n",
      "i1=psis2(i2)=3\n",
      "状态序列I： [3 2 3 2 2 3 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 2, 3, 2, 2, 3, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMM.train(O) \n",
    "print('viterbi:')\n",
    "HMM.viterbi(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "参考代码：https://blog.csdn.net/tudaodiaozhale\n",
    "\n",
    "代码全部测试通过。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
